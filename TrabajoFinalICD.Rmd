---
title: "Trabajo Final Introducción a la Ciencia de Datos"
author: "Juanjo Sierra"
date: "27 de diciembre de 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Planteamiento

El trabajo final de la asignatura _Introducción a la Ciencia de Datos_ se divide en dos secciones. Consiste en realizar un estudio sobre un conjunto de datos de regresión y otro sobre un conjunto de datos de clasificación. Se aplicarán distintas técnicas aprendidas durante la asignatura para conseguir los resultados adecuados.

# Librerías y paquetes a cargar

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(kknn)
```


# Trabajo Final Regresión

En primer lugar se realizará el estudio de la base de datos de regresión. En este caso el conjunto de datos a analizar es **Friedman**, que se ha descargado desde el repositorio de datasets de la asignatura. Se puede leer utilizando la siguiente orden:

```{r}
friedman = read.csv("Datos/friedman/friedman.dat", header = FALSE, comment.char = "@")
head(friedman)
```

Dado que los nombres asignados a las variables no aportan ninguna información, y en el resumen del dataset en formato KEEL podemos comprobar que sus nombres tampoco son representativos, se procede a asignarles una notación genérica.

```{r}
n = length(names(friedman))-1
names(friedman)[1:n] = paste ("X", 1:n, sep="")
names(friedman)[n+1] = "Y"
head(friedman)
```

Ahora podemos comprobar de forma más directa que existen 5 variables de entrada (X1-5) que determinan una única variable de salida (Y). Es interesante comprobar las dimensiones del dataset para poder asegurar que se está asumiendo lo correcto.

```{r}
dim(friedman)
```

Con esto se puede confirmar que existen un total de 1200 ejemplos en el conjunto de datos, cada uno con 6 variables (5 de entrada y 1 de salida).

Utilizando la función `summary` se puede obtener una visión más completa del dataset, arrojando nuevos valores interesantes para su estudio como los rangos de las variables, sus cuartiles o su media y mediana.

```{r}
summary(friedman)
```

Se puede comprobar también si existen valores perdidos en el dataset. Para esto se puede utilizar la función `anyNA`:

```{r}
anyNA(friedman)
```

Este resultado indica que no hay valores perdidos y que por lo tanto no es necesario imputar ni tomar ninguna decisión para restablecer dichos valores.

A continuación se puede comprobar si existen ejemplos duplicados, y eliminarlos del dataset. Para ello se utiliza la función `duplicated` acompañado de la función `any`:

```{r}
any(duplicated(friedman))
```

Como no hay duplicados se puede continuar con el estudio sin realizar ninguna alteración en el conjunto de datos.

Además, como el rango de las variables está entre 0 y 1 (como se pudo comprobar anteriormente con el `summary`), no es necesario realizar un escalado ni una transformación en los valores. En este punto se puede afirmar que los datos están listos para poder trabajar con ellos.

Como primer paso para el análisis del dataset se puede mostrar cada una de las variables de entrada con respecto a la variable de salida. Esto permitirá averiguar de un vistazo cuál tiene más potencial de determinar qué valor de salida obtendrá dicho ejemplo.

```{r}
plotY = function (x,y) {
  plot(friedman[,y]~friedman[,x], xlab=names(friedman)[x], ylab=names(friedman)[y])
}

par(mfrow=c(2,3))
x = sapply(1:(dim(friedman)[2]-1), plotY, dim(friedman)[2])
par(mfrow=c(1,1))
```

Basándose en un modelo de regresión lineal, se puede especular que la variable X4 parece tener una correlación más alta con la variable de salida Y, y por tanto podría resultar en un mejor modelo. A continuación se muestra la gráfica de X4 frente a Y más grande para poder apreciar mejor la posible correlación.

```{r}
plotY(4,dim(friedman)[2])
```

La correlación de las variables entre sí y con la variable de salida puede obtenerse de forma directa gracias a la función `cor`:

```{r}
cor(friedman)
```

Como se había supuesto anteriormente en función de las gráficas obtenidas, es la variable X4 la que más correlación tiene con la variable de salida Y (~0.616).

## Modelos de regresión lineal simple

El primer objetivo del trabajo final de regresión es generar un modelo lineal con cada una de las variables de entrada del dataset. De esta forma se puede obtener de una manera sencilla la información sobre qué variable es mejor para un modelo lineal, es decir, qué variable es más representativa de la de salida.

Para realizar los modelos de regresión lineal se va a utilizar la función `lm` que ya viene entre las funciones base de R. Es necesario indicar cuál es la variable de salida y cuál (o cuáles) son las que se van a utilizar para construir el modelo.

Se van a analizar todas las variables X con respecto a la variable de salida Y. Se construye un modelo con cada una de estas variables, y con la función `summary` se obtiene una información más detallada del modelo resultante.

```{r}
lmsimple1 = lm(Y~X1, data=friedman)
lmsimple2 = lm(Y~X2, data=friedman)
lmsimple3 = lm(Y~X3, data=friedman)
lmsimple4 = lm(Y~X4, data=friedman)
lmsimple5 = lm(Y~X5, data=friedman)

summary(lmsimple1)
summary(lmsimple2)
summary(lmsimple3)
summary(lmsimple4)
summary(lmsimple5)
```

De los resultados anteriores se pueden extraer varias afirmaciones. En primer lugar, el p-value de los modelos de X1, X2, X4 y X5 es muy pequeño (< 2.2e-16) por lo que se puede afirmar con una confianza casi cercana al 100% que existe algún tipo de dependencia lineal entre dichas variables y la variable de salida. En el caso de la variable X3, sin embargo, el p-value es muy alto (> 0.2) por lo que no se puede afirmar lo anterior con suficiente confianza, es decir, es una variable que no se utilizará generalmente para construir un modelo de regresión lineal.

De entre los modelos aceptables, como era de esperar el mejor es el que utiliza X4, la variable que más correlación mantiene con la salida, a pesar de ser "tan sólo" un valor de R-cuadrado de 0.379. El R-squared o R-cuadrado indica cómo de bueno es el modelo de regresión lineal. Cuanto más próximo a 1 más acertado es, y cuanto más próximo a 0 al contrario. Es por esto que a pesar de que el valor de X4 no es muy óptimo, es el que más cerca se encuentra del 1, y por tanto el mejor de las variables estudiadas.

# Modelos de regresión lineal múltiple

A continuación se va a intentar llegar a un modelo de regresión linear múltiple que obtenga mejores resultados que el modelo anterior. Para ello se construirá un modelo con todas las variables de entrada posibles y se eliminarán las menos prometedoras para encontrar el mejor balance entre complejidad y acierto.

Lo primero es construir el modelo con todas las variables, como se ha indicado anteriormente.

```{r}
lmmultiple1 = lm(Y~., data=friedman)
summary(lmmultiple1)
```

Se puede observar que este modelo, siendo más complejo, es también sustancialmente mejor que cualquier modelo de regresión lineal simple. Este modelo alcanza un valor de R-cuadrado ajustado de casi 0.73, mientras que el mejor de los anteriores tan sólo llegaba a 0.379. Sin embargo, comparando los p-values de los diferentes atributos se puede observar que el de X3 es muy elevado, al igual que evaluándolos individualmente. Es por ello que el siguiente paso es eliminarlo del modelo.

```{r}
lmmultiple2 = lm(Y~.-X3, data=friedman)
summary(lmmultiple2)
```

Este modelo incluso da un R-cuadrado ajustado mayor que el que contenía todas las variables, por lo que es intrínsecamente mejor tanto en complejidad como en acierto. Ahora todas las variables tienen un p-value ínfimo por lo que no se puede elegir una clara que eliminar para seguir probando a hacer un modelo más simple. Por ello, se va a eliminar la que menor R-cuadrado tuviese en los modelos lineales simples. En este caso, X5 (0.07).

```{r}
lmmultiple3 = lm(Y~.-X3-X5, data=friedman)
summary(lmmultiple3)
```

El modelo resultante desciende su valor de R-cuadrado ajustado hasta 0.653, casi un 0.1 con respecto al modelo anterior. Teniendo esto en cuenta, se puede afirmar que la reducción de complejidad eliminando variables en este caso no compensa ya que se pierde demasiado acierto con respecto al modelo inmediatamente superior en complejidad. El mejor modelo lineal múltiple hasta el momento es `lmmultiple2`.

Una vez se ha llegado a esta conclusión, se pueden realizar transformaciones sobre las variables o interacciones entre ellas para tratar de obtener un modelo más preciso.

### Interacciones

En primer lugar se realizarán algunos modelos basados en interacciones entre las variables más prometedoras del dataset, con el objetivo de buscar posibles combinaciones que arrojen un modelo más adecuado al problema.

Para empezar se pueden probar las dos variables que, individualmente, han resultado más adecuadas para realizar un modelo lineal.

```{r}
lminteraccion1 = lm(Y~X4*X1, data=friedman)
summary(lminteraccion1)
```

El modelo tan sólo llega a un R-cuadrado de 0.52, y el p-value del producto X4*X1 no garantiza una alta confianza de que dicho valor resulte significativo para la predicción de la variable de salida Y. El siguiente modelo será, por tanto, el que combine lo anterior con una nueva variable, la X2, que es la siguiente en la lista de prometedoras.

```{r}
lminteraccion2 = lm(Y~X4*X1*X2, data=friedman)
summary(lminteraccion2)
```

Este modelo ya es sustancialmente mejor, llegando hasta un R-cuadrado ajustado de 0.656. La confianza de la combinación de X1, X2 y X4 es superior a un 97% por lo que no se puede rechazar la hipótesis de que tenga una correlación lineal con la variable de salida. A pesar de que haya valores más altos de p-value para interacciones que son un subconjunto de la interacción principal, es el p-value de la última el que indica si es confiable.

Dado que el modelo de regresión lineal múltiple que mejor resultado ha dado ha sido el que combinaba todas las variables menos X3, se puede añadir X5, la variable que falta, y comprobar cómo se integra en el modelo.

```{r}
lminteraccion3 = lm(Y~X4*X1*X2*X5, data=friedman)
summary(lminteraccion3)
```

Este modelo supone un caso particular, y es que su acierto es superior a la de los modelos anteriores (R-cuadrado superior a 0.73), pero a su vez no refleja una confianza que garantice que la interacción estudiada se ajuste de forma lineal a la variable de salida. Por dicha razón, mientras se esté buscando un modelo de regresión lineal no se va a utilizar este. De nuevo, la falta de confianza viene dada por un p-valor muy alto (~0.706).

### No linealidad

El siguiente paso es tratar de encontrar si alguna de las combinaciones que se han probado tienen un componente no lineal que pueda ayudar a ajustar mejor. En primer lugar se probará con X4, la mejor variable encontrada mediante modelos de regresión lineal simples, dado que ninguna variable aparenta seguir una distribución cuadrática con respecto a la variable de salida.

```{r}
lmnolinealidad1 = lm(Y~X4+I(X4^2), data=friedman)
summary(lmnolinealidad1)
```

El modelo es ligeramente mejor al lineal simple, pero como se puede observar en el resumen, el p-valor de la variable al cuadrado es sustancialmente mayor al de la varaible simple. Por tanto no merece la pena investigar esa vertiente. Se puede probar con otra variable distinta, como X1.

```{r}
lmnolinealidad2 = lm(Y~X1+I(X1^2), data=friedman)
summary(lmnolinealidad2)
```

Dado que el p-valor de la variable cuadrática es pequeño, puede afirmarse con una alta confianza que X1^2 tiene dependencia lineal con Y, y por tanto puede ser útil en modelos más complejos. Se puede probar a añadirla al mejor modelo hasta ahora, el que utilizaba todas las variables menos X3.

```{r}
lmnolinealidad3 = lm(Y~.-X3+I(X1^2), data=friedman)
summary(lmnolinealidad3)
```

Se ha encontrado un modelo en el que una transformación no lineal ha aportado una solución considerablemente mejor a la anterior. El R-cuadrado ajustado ha pasado de 0.72 a 0.76, y el p-valor de todas las variables que componen el modelo asegura con confianza su correlación lineal con la variable Y. Por tanto este modelo supera a `lmmultiple2` como mejor modelo encontrado.

## Algoritmo k-NN para regresión no paramétrica

Para el siguiente apartado del trabajo final de regresión se va a realizar un estudio del dataset con el algoritmo k-NN. Se probarán distintas configuraciones para encontrar la mejor combinación de variables en el caso de dicho algoritmo.

Cabe destacar que los datos utilizados por k-NN deben estar normalizados, para que las variables con valores de mayor orden no tengan más influencia en el cálculo de la distancia que aquellas con valores de menor orden. En el caso de este dataset, las variables de entrada tienen los valores entre 0 y 1 por lo que no es necesario realizar el escalado. Aunque, por defecto, la normalización se hace gracias al parámetro `scale = TRUE` de la función `kknn`. Para utilizar dicha función es necesario cargar el paquete homónimo, como ya se hizo al comienzo del trabajo.

En primer lugar se va a partir del modelo que contiene todas las variables. Los valores por defecto que se utilizan son k=7, y el kernel óptimo. Se utiliza el mismo conjunto de datos (el dataset entero) tanto para train como para test en este caso, para obtener un único valor que ejemplifique cómo se comporta el modelo para train.

```{r}
fitknn1 = kknn(Y~., friedman, friedman)
```

A continuación se muestran los valores reales de Y frente a los que k-NN ha predicho, para comprobar de forma visual cuánto se parece la predicción a la realidad.

```{r}
plot(friedman$Y~friedman$X4)
points(friedman$X4, fitknn1$fitted.values, col="red")
```

De esta gráfica sólo se puede intuir que el modelo construido con k-NN calcula unos valores de Y que siguen una distribución similar a la de la variable real. Sin embargo, se necesita una medida objetiva que ayude a valorar estos modelos de forma más precisa. Para ello se utiliza la raíz del error cuadrático medio (Root-Mean-Square Error, o __RMSE__). Se puede calcular siguiendo la siguiente fórmula:

```{r}
RMSE = function(fit, labels) {
  yprime = fit$fitted.values
  sqrt(sum((labels-yprime)^2)/length(yprime)) # RMSE
}

RMSE(fitknn1, friedman$Y)
```

Ahora que se ha implementado la función del cálculo del RMSE, se pueden generar nuevos modelos con el algoritmo k-NN y compararlos entre sí. Por ejemplo, el modelo que contiene todas las variables menos X3, que anteriormente se ha comprobado que no ha aportado nada.

```{r}
fitknn2 = kknn(Y~.-X3, friedman, friedman)
RMSE(fitknn2, friedman$Y)
```

En este caso se puede comprobar que los mejores modelos para regresión lineal no tienen por qué ser los mejores para el algoritmo k-NN. Aun así, se puede probar la selección de características del mejor modelo de regresión lineal múltiple para k-NN y de esta forma valorar cómo de bien aproxima la variable de salida con respecto a los otros modelos de este algoritmo.

```{r}
fitknn3 = kknn(Y~.-X3+I(X1^2), friedman, friedman)
RMSE(fitknn3, friedman$Y)
```

La adición de la variable X1 cuadrática ha aportado al modelo, aunque sin embargo este sigue sin ser mejor que el que tiene todas las variables. Si la añadimos a dicho modelo se podrá comprobar si realmente es una buena característica a tener en cuenta.

```{r}
fitknn4 = kknn(Y~.+I(X1^2), friedman, friedman)
RMSE(fitknn4, friedman$Y)
```

El modelo es mejor que el mejor obtenido previamente pero no es una mejora sustancial a cambio de aumentar la complejidad con una nueva variable. Se puede comprobar si eliminando alguna otra variable el RMSE disminuye (X1 no puede ser eliminada porque es necesaria para el cálculo de X1^2).

```{r}
fitknn5 = kknn(Y~.-X2+I(X1^2), friedman, friedman)
fitknn6 = kknn(Y~.-X4+I(X1^2), friedman, friedman)
fitknn7 = kknn(Y~.-X5+I(X1^2), friedman, friedman)
RMSE(fitknn5, friedman$Y)
RMSE(fitknn6, friedman$Y)
RMSE(fitknn7, friedman$Y)
```

Los resultados reflejan que no merece la pena eliminar ninguna de las variables, y que el mejor modelo según el criterio seguido en el estudio sería el que contiene todas las variables, ya que añadir la variable X1 cuadrática no aporta una suficiente mejora, y sin embargo sí añade complejidad al modelo.