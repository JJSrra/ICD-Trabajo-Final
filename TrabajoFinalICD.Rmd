---
title: "Trabajo Final Introducción a la Ciencia de Datos"
author: "Juanjo Sierra"
date: "27 de diciembre de 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Planteamiento

El trabajo final de la asignatura _Introducción a la Ciencia de Datos_ se divide en dos secciones. Consiste en realizar un estudio sobre un conjunto de datos de regresión y otro sobre un conjunto de datos de clasificación. Se aplicarán distintas técnicas aprendidas durante la asignatura para conseguir los resultados adecuados.

# Librerías y paquetes a cargar

```{r}
library(scales)
library(dummies)
library(ggplot2)
library(dplyr)
library(tidyr)
library(caret)
library(kknn)
```

# Análisis de los datos

En este primer apartado se estudiarán la distribución de los datos, su rango de valores, y toda aquella información que se considere necesaria para la correcta aplicación del trabajo correspondiente a cada parte.

## Análisis para regresión

En primer lugar se realizará el estudio de la base de datos de regresión. En este caso el conjunto de datos a analizar es **Friedman**, que se ha descargado desde el repositorio de datasets de la asignatura. Se puede leer utilizando la siguiente orden:

```{r}
friedman = read.csv("Datos/friedman/friedman.dat", header = FALSE, comment.char = "@")
head(friedman)
```

Dado que los nombres asignados a las variables no aportan ninguna información, y en el resumen del dataset en formato KEEL podemos comprobar que sus nombres tampoco son representativos, se procede a asignarles una notación genérica.

```{r}
n = length(names(friedman))-1
names(friedman)[1:n] = paste ("X", 1:n, sep="")
names(friedman)[n+1] = "Y"
head(friedman)
```

Ahora podemos comprobar de forma más directa que existen 5 variables de entrada (X1-5) que determinan una única variable de salida (Y). Es interesante comprobar las dimensiones del dataset para poder asegurar que se está asumiendo lo correcto.

```{r}
dim(friedman)
```

Con esto se puede confirmar que existen un total de 1200 ejemplos en el conjunto de datos, cada uno con 6 variables (5 de entrada y 1 de salida).

Antes de nada se debe comprobar si existen valores perdidos en el dataset. Para esto se puede utilizar la función `anyNA`:

```{r}
anyNA(friedman)
```

Este resultado indica que no hay valores perdidos y que por lo tanto no es necesario imputar ni tomar ninguna decisión para restablecer dichos valores.

Utilizando la función `summary` se puede obtener una visión más completa del dataset, arrojando nuevos valores interesantes para su estudio como los rangos de las variables, sus cuartiles o su media y mediana.

```{r}
summary(friedman)
```

Se puede calcular la desviación típica utilizando la función `sd`:

```{r}
sapply(friedman, sd)
```

Mediante una gráfica box-plot se puede comprobar si los datos están escalados o centrados, y valorar cómo se comparan sus varianzas. Utilizando la función `var` se calculan dichas varianzas.

```{r}
boxplot(friedman[1:5])
sapply(friedman[1:5], var)
```

En función de la gráfica obtenida, se puede valorar que los datos están más o menos centrados y escalados entre 0 y 1, y que sus varianzas son muy similares. Esto se puede comprobar con los resultados reflejados por la función anteriormente citada.

A continuación se puede comprobar si existen ejemplos duplicados, y eliminarlos del dataset. Para ello se utiliza la función `duplicated` acompañado de la función `any`:

```{r}
any(duplicated(friedman))
```

Como no hay duplicados se puede continuar con el estudio sin realizar ninguna alteración en el conjunto de datos.

Se puede comprobar la distribución de los datos utilizando histogramas. En este caso se utilizan simultáneamente las librerías `ggplot2`y `tidyr`.

```{r}
friedman %>% gather() %>% ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") + geom_histogram(bins = 20)
```

No parece que las variables sigan una distribución normal, pero pese a todo no está de más comprobarlo estadísticamente. Se puede estudiar la normalidad de los datos utilizando `qqnorm` y `qqline` para dar una primera visión de cómo se aproximan a una normal. Igualmente, se debe realizar el test de Shapiro-Wilk para contrastar la normalidad o no de los datos.

```{r}
qqnorm(friedman$X1)
qqline(friedman$X1)

qqnorm(friedman$X2)
qqline(friedman$X2)

qqnorm(friedman$X3)
qqline(friedman$X3)

qqnorm(friedman$X4)
qqline(friedman$X4)

qqnorm(friedman$X5)
qqline(friedman$X5)
```

En base a las gráficas no se puede concluir de forma clara si las variables se ajustan a una distribución normal por lo que es necesario validar con el test de Shapiro-Wilk.

```{r}
sapply(friedman[1:5], shapiro.test)
```

Los p-valores tan bajos que se obtienen de cada una de las variables con el test de Shapiro-Wilk provocan que se rechace la hipótesis de que dichas variables siguen una distribución normal.

Además, como el rango de las variables está entre 0 y 1 (como se pudo comprobar anteriormente con el `summary` y el box-plot), no es necesario realizar un escalado ni una transformación en los valores. En este punto se puede afirmar que los datos están listos para poder trabajar con ellos.

Para continuar con el análisis del dataset se puede mostrar una gráfica comparativa de cada variable con respecto del resto, así se puede visualizar cuáles tienen una correlación alta.

```{r}
plot(friedman)
```

Visualizando la gráfica anterior se aprecia que entre las variables de entrada no existe ninguna correlación clara, de hecho los puntos se distribuyen casi por todo el espacio de la gráfica. Las que sí tienen más interés son aquellas que comparan las distintas variables de entrada con la variable de salida Y. Ver directamente estas gráficas permitirá averiguar de un vistazo cuál tiene más potencial de determinar qué valor de salida obtendrá dicho ejemplo.

```{r}
plotY = function (x,y) {
  plot(friedman[,y]~friedman[,x], xlab=names(friedman)[x], ylab=names(friedman)[y])
}

par(mfrow=c(2,3))
x = sapply(1:(dim(friedman)[2]-1), plotY, dim(friedman)[2])
par(mfrow=c(1,1))
```

Basándose en un modelo de regresión lineal, se puede especular que la variable X4 parece tener una correlación más alta con la variable de salida Y, y por tanto podría resultar en un mejor modelo. A continuación se muestra la gráfica de X4 frente a Y más grande para poder apreciar mejor la posible correlación.

```{r}
plotY(4,dim(friedman)[2])
```

La correlación de las variables entre sí y con la variable de salida puede obtenerse de forma directa gracias a la función `cor`:

```{r}
cor(friedman)
```

Como se había supuesto anteriormente en función de las gráficas obtenidas, es la variable X4 la que más correlación tiene con la variable de salida Y (~0.616).

## Análisis para clasificación

El conjunto de datos que se va a utilizar para el estudio de clasificación es **Australian**, también descargado desde el repositorio de datasets correspondiente a la asignatura. Se puede leer el dataset completo mediante la siguiente orden:

```{r}
australian = read.csv("Datos/australian/australian.dat", header = FALSE, comment.char = "@")
head(australian)
```

En este caso se comprueba que existen muchas variables cuyo tipo atómico es entero, pero tan sólo algunas resultan ser variables categóricas. Más adelante se verá cuáles son.

De nuevo los nombres asignados a las variables no aportan ninguna información, y ya que en el resumen en formato KEEL correspondiente al dataset podemos comprobar que sus nombres no aportan ninguna información, se procede a asignarles una notación genérica.

```{r}
n = length(names(australian))-1
names(australian)[1:n] = paste ("X", 1:n, sep="")
names(australian)[n+1] = "Y"
head(australian)
```

Para comprobar que se ha leído de forma correcta el dataset, se pueden estudiar sus dimensiones y asegurarse de que hay 690 ejemplos con 14 variables de entrada y una de salida.

```{r}
dim(australian)
```

Antes de continuar con el resto del estudio del dataset debe asegurarse que no contiene valores perdidos. En caso de haberlos, se deberá tomar una decisión adecuada para solventar la situación.

```{r}
anyNA(australian)
```

Como no existen valores perdidos en el dataset no hay que hacer ningún cálculo para imputarlos, ni es necesario tomar una decisión al respecto. Ahora sí se puede utlizar `summary` para obtener una visión general del dataset, para averiguar sus cuartiles, su media y mediana y el rango de cada variable.

```{r}
summary(australian)
```

El resumen del dataset muestra los valores anteriormente citados. Aquí puede observarse qué variables corresponden a datos reales y cuáles a enteros, y entre ellas, se puede apreciar mejor cuáles corresponden a un valor booleano (rango 0-1) y cuáles son variables categóricas con más valores (por ejemplo X12, que toma los valores 1, 2 y 3).

Es interesante saber cuántos valores de clase puede tomar la variable de salida Y, para saber si el dataset está balanceado o no.

```{r}
table(australian$Y)
```

Dados estos datos se puede afirmar que está bastante equilibrada, dado que sólo hay dos clases posibles y toman en torno al 50% de los datos cada una.

Un dato que no aparece en el resumen y que se puede calcular es la desviación típica. Para ello se utiliza la función `sd`:

```{r}
sapply(australian, sd)
```

Con una gráfica box-plot pueden verse de forma gráfica los cuartiles de las variables, ademaś de sus varianzas y lo centrados o escalados que están los datos. Sin embargo, como los valores para este dataset no están normalizados es de esperar que la gráfica no aporte mucha información. Sin embargo, para calcular directamente las varianzas se puede utilizar la función `var`:

```{r}
boxplot(australian[1:14])
sapply(australian[1:14], var)
```

Utilizando `tidyr` y `ggplot` se pueden ver estos histogramas que aporten una idea general de la distribución de cada una de las variables del dataset. 

```{r}
australian %>% gather() %>% ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") + geom_histogram(bins = 20)
```

Para obtener de forma más precisa una conclusión con respecto a la normalidad de los datos, se debe realizar el test de Shapiro-Wilk sobre las variables a estudiar. En este caso no tiene sentido sobre las variables categóricas, así que se hará sobre las que no lo son.

```{r}
sapply(australian[c(2,3,5,7,10,13,14)], shapiro.test)
```

Como el p-valor es muy bajo en todas, se puede afirmar con una confianza muy alta (superior al 99%) que dichas variables no siguen una distribución normal.

Con respecto a las variables dummy, para este dataset sólo se van a aplicar a aquellas variables que deberían ser consideradas factor a pesar de haber sido leídas como entero. Se trata de las variables X4 y X12, que solamente toman valores 1, 2 y 3. Para no darle más peso a las diferencias entre distintos pares de valores, se toman valores dummy.

```{r}
australian$X4 = as.factor(australian$X4)
australian$X12 = as.factor(australian$X12)

australian = dummy.data.frame(australian, sep=".")
head(australian)
```

Como para aplicar el algoritmo k-NN va a ser necesario normalizar los datos, se proceden a escalar a continuación.

```{r}
maxs = apply(australian, 2, max)
mins = apply(australian, 2, min)
australian = as.data.frame(scale(australian, center = mins, scale = maxs - mins))
head(australian)
```

Como existen tantas variables es complicado realizar una gráfica comparativa de cada variable con respecto al resto y que esta aporte información útil, por lo que se muestran directamente los valores de correlación utilizando la función `cor`. Como la matriz es enorme se obtiene un subconjunto de aquellos valores superiores a 0.5 y que no sean el 1.0 correspondiente a la diagonal. 

```{r}
correlation = as.data.frame(as.table(cor(australian)))
subset(correlation, abs(Freq) > 0.5 & abs(Freq) < 1.0)
```

Cabe destacar la alta correlación negativa entre X4.2 y X4.1, obviamente esto se debe a que es una variable categórica que no puede tomar ambos valores a la vez, y el tercer valor apenas está presente. Por tanto es un indicativo de que una de las variables se puede eliminar por "repetir información" y estar muy ligadas. Lo mismo ocurre con X12.2 y X12.1.

```{r}
australian = australian[,-which(names(australian) %in% c("X4.1", "X12.1"))]
```

# Trabajo Final Regresión

## Modelos de regresión lineal simple

El primer objetivo del trabajo final de regresión es generar un modelo lineal con cada una de las variables de entrada del dataset. De esta forma se puede obtener de una manera sencilla la información sobre qué variable es mejor para un modelo lineal, es decir, qué variable es más representativa de la de salida.

Para realizar los modelos de regresión lineal se va a utilizar la función `lm` que ya viene entre las funciones base de R. Es necesario indicar cuál es la variable de salida y cuál (o cuáles) son las que se van a utilizar para construir el modelo.

Se van a analizar todas las variables X con respecto a la variable de salida Y. Se construye un modelo con cada una de estas variables, y con la función `summary` se obtiene una información más detallada del modelo resultante.

```{r}
lmsimple1 = lm(Y~X1, data=friedman)
lmsimple2 = lm(Y~X2, data=friedman)
lmsimple3 = lm(Y~X3, data=friedman)
lmsimple4 = lm(Y~X4, data=friedman)
lmsimple5 = lm(Y~X5, data=friedman)

summary(lmsimple1)
summary(lmsimple2)
summary(lmsimple3)
summary(lmsimple4)
summary(lmsimple5)
```

De los resultados anteriores se pueden extraer varias afirmaciones. En primer lugar, el p-value de los modelos de X1, X2, X4 y X5 es muy pequeño (< 2.2e-16) por lo que se puede afirmar con una confianza casi cercana al 100% que existe algún tipo de dependencia lineal entre dichas variables y la variable de salida. En el caso de la variable X3, sin embargo, el p-value es muy alto (> 0.2) por lo que no se puede afirmar lo anterior con suficiente confianza, es decir, es una variable que no se utilizará generalmente para construir un modelo de regresión lineal.

De entre los modelos aceptables, como era de esperar el mejor es el que utiliza X4, la variable que más correlación mantiene con la salida, a pesar de ser "tan sólo" un valor de R-cuadrado de 0.379. El R-squared o R-cuadrado indica cómo de bueno es el modelo de regresión lineal. Cuanto más próximo a 1 más acertado es, y cuanto más próximo a 0 al contrario. Es por esto que a pesar de que el valor de X4 no es muy óptimo, es el que más cerca se encuentra del 1, y por tanto el mejor de las variables estudiadas.

# Modelos de regresión lineal múltiple

A continuación se va a intentar llegar a un modelo de regresión linear múltiple que obtenga mejores resultados que el modelo anterior. Para ello se construirá un modelo con todas las variables de entrada posibles y se eliminarán las menos prometedoras para encontrar el mejor balance entre complejidad y acierto.

Lo primero es construir el modelo con todas las variables, como se ha indicado anteriormente.

```{r}
lmmultiple1 = lm(Y~., data=friedman)
summary(lmmultiple1)
```

Se puede observar que este modelo, siendo más complejo, es también sustancialmente mejor que cualquier modelo de regresión lineal simple. Este modelo alcanza un valor de R-cuadrado ajustado de casi 0.73, mientras que el mejor de los anteriores tan sólo llegaba a 0.379. Sin embargo, comparando los p-values de los diferentes atributos se puede observar que el de X3 es muy elevado, al igual que evaluándolos individualmente. Es por ello que el siguiente paso es eliminarlo del modelo.

```{r}
lmmultiple2 = lm(Y~.-X3, data=friedman)
summary(lmmultiple2)
```

Este modelo incluso da un R-cuadrado ajustado mayor que el que contenía todas las variables, por lo que es intrínsecamente mejor tanto en complejidad como en acierto. Ahora todas las variables tienen un p-value ínfimo por lo que no se puede elegir una clara que eliminar para seguir probando a hacer un modelo más simple. Por ello, se va a eliminar la que menor R-cuadrado tuviese en los modelos lineales simples. En este caso, X5 (0.07).

```{r}
lmmultiple3 = lm(Y~.-X3-X5, data=friedman)
summary(lmmultiple3)
```

El modelo resultante desciende su valor de R-cuadrado ajustado hasta 0.653, casi un 0.1 con respecto al modelo anterior. Teniendo esto en cuenta, se puede afirmar que la reducción de complejidad eliminando variables en este caso no compensa ya que se pierde demasiado acierto con respecto al modelo inmediatamente superior en complejidad. El mejor modelo lineal múltiple hasta el momento es `lmmultiple2`.

Una vez se ha llegado a esta conclusión, se pueden realizar transformaciones sobre las variables o interacciones entre ellas para tratar de obtener un modelo más preciso.

### Interacciones

En primer lugar se realizarán algunos modelos basados en interacciones entre las variables más prometedoras del dataset, con el objetivo de buscar posibles combinaciones que arrojen un modelo más adecuado al problema.

Para empezar se pueden probar las dos variables que, individualmente, han resultado más adecuadas para realizar un modelo lineal.

```{r}
lminteraccion1 = lm(Y~X4*X1, data=friedman)
summary(lminteraccion1)
```

El modelo tan sólo llega a un R-cuadrado de 0.52, y el p-value del producto X4*X1 no garantiza una alta confianza de que dicho valor resulte significativo para la predicción de la variable de salida Y. El siguiente modelo será, por tanto, el que combine lo anterior con una nueva variable, la X2, que es la siguiente en la lista de prometedoras.

```{r}
lminteraccion2 = lm(Y~X4*X1*X2, data=friedman)
summary(lminteraccion2)
```

Este modelo ya es sustancialmente mejor, llegando hasta un R-cuadrado ajustado de 0.656. La confianza de la combinación de X1, X2 y X4 es superior a un 97% por lo que no se puede rechazar la hipótesis de que tenga una correlación lineal con la variable de salida. A pesar de que haya valores más altos de p-value para interacciones que son un subconjunto de la interacción principal, es el p-value de la última el que indica si es confiable.

Dado que el modelo de regresión lineal múltiple que mejor resultado ha dado ha sido el que combinaba todas las variables menos X3, se puede añadir X5, la variable que falta, y comprobar cómo se integra en el modelo.

```{r}
lminteraccion3 = lm(Y~X4*X1*X2*X5, data=friedman)
summary(lminteraccion3)
```

Este modelo supone un caso particular, y es que su acierto es superior a la de los modelos anteriores (R-cuadrado superior a 0.73), pero a su vez no refleja una confianza que garantice que la interacción estudiada se ajuste de forma lineal a la variable de salida. Por dicha razón, mientras se esté buscando un modelo de regresión lineal no se va a utilizar este. De nuevo, la falta de confianza viene dada por un p-valor muy alto (~0.706).

### No linealidad

El siguiente paso es tratar de encontrar si alguna de las combinaciones que se han probado tienen un componente no lineal que pueda ayudar a ajustar mejor. En primer lugar se probará con X4, la mejor variable encontrada mediante modelos de regresión lineal simples, dado que ninguna variable aparenta seguir una distribución cuadrática con respecto a la variable de salida.

```{r}
lmnolinealidad1 = lm(Y~X4+I(X4^2), data=friedman)
summary(lmnolinealidad1)
```

El modelo es ligeramente mejor al lineal simple, pero como se puede observar en el resumen, el p-valor de la variable al cuadrado es sustancialmente mayor al de la varaible simple. Por tanto no merece la pena investigar esa vertiente. Se puede probar con otra variable distinta, como X1.

```{r}
lmnolinealidad2 = lm(Y~X1+I(X1^2), data=friedman)
summary(lmnolinealidad2)
```

Dado que el p-valor de la variable cuadrática es pequeño, puede afirmarse con una alta confianza que X1^2 tiene dependencia lineal con Y, y por tanto puede ser útil en modelos más complejos. Se puede probar a añadirla al mejor modelo hasta ahora, el que utilizaba todas las variables menos X3.

```{r}
lmnolinealidad3 = lm(Y~.-X3+I(X1^2), data=friedman)
summary(lmnolinealidad3)
```

Se ha encontrado un modelo en el que una transformación no lineal ha aportado una solución considerablemente mejor a la anterior. El R-cuadrado ajustado ha pasado de 0.72 a 0.76, y el p-valor de todas las variables que componen el modelo asegura con confianza su correlación lineal con la variable Y. Por tanto este modelo supera a `lmmultiple2` como mejor modelo encontrado.

## Algoritmo k-NN para regresión no paramétrica

Para el siguiente apartado del trabajo final de regresión se va a realizar un estudio del dataset con el algoritmo k-NN. Se probarán distintas configuraciones para encontrar la mejor combinación de variables en el caso de dicho algoritmo.

Cabe destacar que los datos utilizados por k-NN deben estar normalizados, para que las variables con valores de mayor orden no tengan más influencia en el cálculo de la distancia que aquellas con valores de menor orden. En el caso de este dataset, las variables de entrada tienen los valores entre 0 y 1 por lo que no es necesario realizar el escalado. Aunque, por defecto, la normalización se hace gracias al parámetro `scale = TRUE` de la función `kknn`. Para utilizar dicha función es necesario cargar el paquete homónimo, como ya se hizo al comienzo del trabajo.

En primer lugar se va a partir del modelo que contiene todas las variables. Los valores por defecto que se utilizan son k=7, y el kernel óptimo. Se utiliza el mismo conjunto de datos (el dataset entero) tanto para train como para test en este caso, para obtener un único valor que ejemplifique cómo se comporta el modelo para train.

```{r}
fitknn1 = kknn(Y~., friedman, friedman)
```

A continuación se muestran los valores reales de Y frente a los que k-NN ha predicho, para comprobar de forma visual cuánto se parece la predicción a la realidad.

```{r}
plot(friedman$Y~friedman$X4)
points(friedman$X4, fitknn1$fitted.values, col="red")
```

De esta gráfica sólo se puede intuir que el modelo construido con k-NN calcula unos valores de Y que siguen una distribución similar a la de la variable real. Sin embargo, se necesita una medida objetiva que ayude a valorar estos modelos de forma más precisa. Para ello se utiliza la raíz del error cuadrático medio (Root-Mean-Square Error, o __RMSE__). Se puede calcular siguiendo la siguiente fórmula:

```{r}
RMSE = function(fit, labels) {
  yprime = fit$fitted.values
  sqrt(sum((labels-yprime)^2)/length(yprime)) # RMSE
}

RMSE(fitknn1, friedman$Y)
```

Ahora que se ha implementado la función del cálculo del RMSE, se pueden generar nuevos modelos con el algoritmo k-NN y compararlos entre sí. Por ejemplo, el modelo que contiene todas las variables menos X3, que anteriormente se ha comprobado que no ha aportado nada.

```{r}
fitknn2 = kknn(Y~.-X3, friedman, friedman)
RMSE(fitknn2, friedman$Y)
```

En este caso se puede comprobar que los mejores modelos para regresión lineal no tienen por qué ser los mejores para el algoritmo k-NN. Aun así, se puede probar la selección de características del mejor modelo de regresión lineal múltiple para k-NN y de esta forma valorar cómo de bien aproxima la variable de salida con respecto a los otros modelos de este algoritmo.

```{r}
fitknn3 = kknn(Y~.-X3+I(X1^2), friedman, friedman)
RMSE(fitknn3, friedman$Y)
```

La adición de la variable X1 cuadrática ha aportado al modelo, aunque sin embargo este sigue sin ser mejor que el que tiene todas las variables. Si la añadimos a dicho modelo se podrá comprobar si realmente es una buena característica a tener en cuenta.

```{r}
fitknn4 = kknn(Y~.+I(X1^2), friedman, friedman)
RMSE(fitknn4, friedman$Y)
```

El modelo es mejor que el mejor obtenido previamente pero no es una mejora sustancial a cambio de aumentar la complejidad con una nueva variable. Se puede comprobar si eliminando alguna otra variable el RMSE disminuye (X1 no puede ser eliminada porque es necesaria para el cálculo de X1^2).

```{r}
fitknn5 = kknn(Y~.-X2+I(X1^2), friedman, friedman)
fitknn6 = kknn(Y~.-X4+I(X1^2), friedman, friedman)
fitknn7 = kknn(Y~.-X5+I(X1^2), friedman, friedman)
RMSE(fitknn5, friedman$Y)
RMSE(fitknn6, friedman$Y)
RMSE(fitknn7, friedman$Y)
```

Los resultados reflejan que no merece la pena eliminar ninguna de las variables, y que el mejor modelo según el criterio seguido en el estudio sería el que contiene todas las variables, ya que añadir la variable X1 cuadrática no aporta una suficiente mejora, y sin embargo sí añade complejidad al modelo.

## Comparativa de algoritmos

En este último apartado se van a comparar los resultados de los dos modelos de regresión múltiple estudiados (regresión lineal múltiple y k-NN) para comprobar si existen diferencias significativas entre ambos, mediante tests de Wilcoxon, Friedman y Holm. Para los tests que soportan más de dos algoritmos como entrada, se añadirán también los resultados del algoritmo M5 como parte de la comparativa.

En este caso se van a comparar únicamente los modelos que contienen todas las variables, cómo punto de partida igualitario para ambos regresores. Llegados a este punto sí tiene sentido contar con un error de train y test, por lo que se van a utilizar las particiones ubicadas dentro de la carpeta del dataset para realizar validación cruzada. La medida de error será el error cuadrático medio o MSE, que viene determinado por la distancia que existe entre las variables de salida predichas y las verdaderas.

En primer lugar se realizará la validación cruzada y el estudio de la regresión lineal múltiple.

```{r}
path = "./Datos/friedman/friedman"

run_lm_fold = function(i, x, tt = "test") {
  file = paste(x, "-5-", i, "tra.dat", sep="")
  x_tra = read.csv(file, comment.char="@")
  file = paste(x, "-5-", i, "tst.dat", sep="")
  x_tst = read.csv(file, comment.char="@")
  In = length(names(x_tra)) - 1
  names(x_tra)[1:In] = paste ("X", 1:In, sep="")
  names(x_tra)[In+1] = "Y"
  names(x_tst)[1:In] = paste ("X", 1:In, sep="")
  names(x_tst)[In+1] = "Y"
  
  if (tt == "train") {
    test = x_tra
  }
  else {
    test = x_tst
  }
  
  fitMulti = lm(Y~., x_tra)
  yprime = predict(fitMulti, test)
  sum(abs(test$Y-yprime)^2)/length(yprime) # MSE
}

lmMSEtrain = mean(sapply(1:5, run_lm_fold, path, "train"))
lmMSEtest = mean(sapply(1:5, run_lm_fold, path, "test"))

lmMSEtrain
lmMSEtest
```

Como se puede observar en base a los resultados, el modelo de regresión lineal múltiple que contiene a todas las variables obtiene un error cuadrático medio rondando los 7.2, con una ligera mejora en train con respecto a test, como es lógico dado que son los datos en los que se ha basado para construir el modelo.

A continuación se prueba el mismo experimento con el algoritmo k-NN, para ver cómo se comparan sus errores cuadráticos medios con los de regresión lineal.

```{r}
run_knn_fold = function(i, x, tt = "test") {
  file = paste(x, "-5-", i, "tra.dat", sep="")
  x_tra = read.csv(file, comment.char="@")
  file = paste(x, "-5-", i, "tst.dat", sep="")
  x_tst = read.csv(file, comment.char="@")
  In = length(names(x_tra)) - 1
  names(x_tra)[1:In] = paste ("X", 1:In, sep="")
  names(x_tra)[In+1] = "Y"
  names(x_tst)[1:In] = paste ("X", 1:In, sep="")
  names(x_tst)[In+1] = "Y"
  
  if (tt == "train") {
    test = x_tra
  }
  else {
    test = x_tst
  }
  
  fitMulti = kknn(Y~., x_tra, test)
  yprime = fitMulti$fitted.values
  sum(abs(test$Y-yprime)^2)/length(yprime) # MSE
}

knnMSEtrain = mean(sapply(1:5, run_knn_fold, path, "train"))
knnMSEtest = mean(sapply(1:5, run_knn_fold, path, "test"))

knnMSEtrain
knnMSEtest
```

Para el k-NN la diferencia entre los errores de train y test se hace más notable, aumentando desde un 1.423 hasta un 3.196. A pesar de la diferencia entre ambas pruebas, el MSE en test, que es el que realmente es significativo, sigue estando bastante por debajo con respecto a su análogo de regresión lineal. El modelo que contiene todas las variables se ajusta por tanto bastante mejor con el algoritmo k-NN que con regresión lineal.

Antes de comenzar con las comparativas entre los algoritmos, se han de leer las tablas con los errores medios tanto de train como de test para tener los datos disponibles para aplicar los tests estadísticos. Estas tablas deberán contener los nuevos valores obtenidos por regresión lineal y k-NN, sustituyendo a los que aparecían por defecto.

```{r}
resultados = read.csv("Datos/regr_train_alumnos.csv")
tablatra = cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatra) = names(resultados)[2:dim(resultados)[2]]
rownames(tablatra) = resultados[,1]

resultados = read.csv("Datos/regr_test_alumnos.csv")
tablatst = cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatst) = names(resultados)[2:dim(resultados)[2]]
rownames(tablatst) = resultados[,1]
```


### Wilcoxon

A continuación se va a ejecutar el test de Wilcoxon para comprobar si existen diferencias significativas entre ambos modelos. Dado que Wilcoxon trabaja con diferencias de error, en regresión hay que normalizarlo para que se obtengan valores útiles. Se toma como referencia el algoritmo que parece ser mejor (k-NN en este caso) y se compara con el otro (regresión lineal múltiple). Se suma 0.1 porque Wilcoxon no trabaja bien con valores iguales a 0.

```{r}
difs = (tablatst[,1] - tablatst[,2]) / tablatst[,1]
wilc_1_2 = cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1),
                 ifelse (difs>0, abs(difs)+0.1, 0+0.1))
colnames(wilc_1_2) = c(colnames(tablatst)[1], colnames(tablatst)[2])
head(wilc_1_2)
```

Con esta tabla se puede entrar a evaluar ya el test de Wilcoxon como tal.

```{r}
LMvsKNNtst = wilcox.test(wilc_1_2[,1], wilc_1_2[,2],
                         alternative = "two.sided", paired=TRUE)
Rmas = LMvsKNNtst$statistic
pvalue = LMvsKNNtst$p.value
LMvsKNNtst = wilcox.test(wilc_1_2[,2], wilc_1_2[,1],
                         alternative = "two.sided", paired=TRUE)
Rmenos = LMvsKNNtst$statistic
Rmas
Rmenos
pvalue
```

Se puede considerar que no existen diferencias significativas entre ambos algoritmos. El grado de confianza que se tiene de que los algoritmos sean distintos es de apenas un 23.4%, por lo que no se puede validar dicha hipótesis.

### Friedman

Para el test de Friedman se valora la posición (el ranking) de cada algoritmo en cada conjunto de datos, con lo que no es necesario normalizar nada. Como la tabla `tablatst` ya contiene los valores de los algoritmos anteriormente mencionados además del algoritmo M5, se puede utilizar directamente para evaluar el test. Eso sí, la función `friedman.test` recibe una matriz, por lo que sí que es necesario realizar el casting correspondiente de forma previa.

```{r}
test_friedman = friedman.test(as.matrix(tablatst))
test_friedman
```

Dado el p-valor tan bajo que se obtiene como resultado del test de Friedman, se puede afirmar con casi un 99% de confianza que al menos un par de algoritmos son diferenciables entre sí significativamente.

### Holm

Ahora que se tiene certeza de que existe algún par de algoritmos con diferencias significativas, para averiguar cuál o cuáles de estas combinaciones son las que cumplen esta hipótesis se realiza el test de Holm.

```{r}
tam = dim(tablatst)
groups = rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(as.matrix(tablatst),
                     groups, p.adjust = "holm", paired = TRUE)
```

En base a los resultados obtenidos, se puede deducir que existen dos pares de algoritmos diferentes significativamente (p-valor rondando el 0.1 o menor). Sabiendo que, según el orden en el que aparecen en la tabla, el 1 representa regresión lineal, el 2 k-NN y el 3 M5, se obtiene que el algoritmo M5 tiene diferencias significativas con respecto tanto a regresión lineal (~92% de confianza) como a k-NN (~90% de confianza). Los otros dos algoritmos, según la tabla, no pueden ser considerados diferentes, como se había comprobado antes en el test de Wilcoxon. Por tanto, se puede afirmar que todas las diferencias significativas entre estos algoritmos son a favor de M5.

# Trabajo Final Clasificación

## Algoritmo K-NN para clasificación

El primer objetivo del trabajo de la parte de clasificación es ejecutar el algoritmo k-NN con este conjunto de datos. Además, se pide que se estudie cuál es la mejor _k_, para con ello obtener el mejor modelo posible.

### Creación de particiones

Lo primero que hay que hacer es separar el dataset en conjuntos de train y de test, y guardar sus etiquetas en una variable distinta. Estos subconjuntos se utilizarán para llevar a cabo las ejecuciones de los algoritmos. Se fija una semilla para que las ejecuciones posteriores siempre reflejen el mismo resultado.

```{r}
set.seed(1010)
shuffle_ds = sample(dim(australian)[1]) 
eightypct = (dim(australian)[1] * 80) %/% 100
australian[,dim(australian)[2]] = as.factor(australian[,dim(australian)[2]])

```

```{r}
head(australian)
```

```{r}
aust_train = australian[shuffle_ds[1:eightypct], 1:dim(australian)[2]-1]
aust_test = australian[shuffle_ds[(eightypct+1):dim(australian)[1]],1:dim(australian)[2]-1] 

aust_train_labels = australian[shuffle_ds[1:eightypct], dim(australian)[2]]
aust_test_labels = australian[shuffle_ds[(eightypct+1):dim(australian)[1]],dim(australian)[2]]
```

Mediante el paquete `caret` y sus funciones `train` y `predict` se pueden construir modelos y calcular sus valores predichos respectivamente. Se va a utilizar la función `train` con el método `knn` para calcular de forma más sencilla y directa la mejor _k_. Se utilizan únicamente valores impares para forzar el desempate entre clases.

```{r}
knnModel = train(aust_train, aust_train_labels, method="knn",
           metric="Accuracy", tuneGrid = data.frame(.k=seq(1,15,2))) 
knnModel
```

Los resultados obtenidos reflejan que el modelo con k=11 es el que mejor acierto tiene. Sin embargo, en caso de que este produzca sobreaprendizaje, se van a estudiar los 3 mejores k para asegurar un resultado óptimo. En primer lugar se construyen los modelos.

```{r}
mejoresK = knnModel$results$k[order(knnModel$results$Accuracy, decreasing = TRUE)[1:3]]
mejoresK

knnModel.1 = train(aust_train, aust_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[1]))

knnModel.2 = train(aust_train, aust_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[2]))

knnModel.3 = train(aust_train, aust_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[3]))
```

A continuación, se calculan las etiquetas predichas por estos modelos utilizando los datos de test.

```{r}
knnPred.1 = predict(knnModel.1, aust_test)
knnPred.2 = predict(knnModel.2, aust_test)
knnPred.3 = predict(knnModel.3, aust_test)
```

Y una vez obtenidas las etiquetas, se comprueba el acierto logrado por cada uno de los modelos en el conjunto de test.

```{r}
postResample(knnPred.1, aust_test_labels)
postResample(knnPred.2, aust_test_labels)
postResample(knnPred.3, aust_test_labels)
```

Como se puede observar, el mejor modelo, k=11, no sobreaprende, y obtiene un acierto similar al segundo. Además el descenso de acierto en test no es muy grande por lo que se trata de un modelo que se puede considerar aceptable.

Se pueden comparar los resultados de los modelos de un vistazo mostrándolos junto a las etiquetas reales, para valorar gráficamente su acierto.

```{r}
par(mfrow=c(2,2))
plot(aust_test$X13~aust_test$X2,col=knnPred.1, main="Modelo 11-NN")
plot(aust_test$X13~aust_test$X2,col=knnPred.2, main="Modelo 13-NN")
plot(aust_test$X13~aust_test$X2,col=knnPred.3, main="Modelo 9-NN")
plot(aust_test$X13~aust_test$X2,col=aust_test_labels, main="Originales Test")
par(mfrow=c(1,1))
```
